{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc04d8c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lab 5: Port Detection using AIS Data\n",
    "\n",
    "This notebook implements a port detection algorithm using Automatic Identification System (AIS) vessel data. The approach involves:\n",
    "1. Parallel data loading and filtering\n",
    "2. Speed calculation and low-speed point identification\n",
    "3. Batch grouping of low-speed areas\n",
    "4. Clustering to identify ports\n",
    "5. Port visualization with statistics\n",
    "\n",
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e669f64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: ../../lab1/data/aisdk-2025-02-09.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import folium\n",
    "from sklearn.cluster import DBSCAN\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data file path\n",
    "file_path = '../../lab1/data/aisdk-2025-02-09.csv'\n",
    "print(f\"Loading data from: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6885be9",
   "metadata": {},
   "source": [
    "## Filter out the noise and prepare data for port detection. (Think what is the noise in this solution)\n",
    "\n",
    "- Mobile units only\n",
    "- Remove the sudden jump points\n",
    "- for each MMSI\n",
    "    - Sort by time\n",
    "    - Calculate the distance, time difference and speeed between points\n",
    "    - Remove all points with speed more than 5 kmh\n",
    "    - Remove all points that have 0 time difference, but a distance of more than 10 metres\n",
    "    - Combine the points into batches - traverse the filtered points, and group them together if they are nearby in terms of space and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "409ae367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "EARTH_RADIUS = 6378  # Earth's radius in kilometers\n",
    "SUDDEN_JUMP_THRESHOLD = 0.01  # Distance threshold for sudden jumps in km\n",
    "LOW_SPEED_THRESHOLD = 5  # Low speed threshold for port detection in km/h\n",
    "MINIMUM_POINTS_FOR_BATCH = 10\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the great circle distance between two points on Earth.\"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return EARTH_RADIUS * c\n",
    "\n",
    "def include_spacetime_features(vessel_data):\n",
    "    # Calculate speeds using vectorized operations\n",
    "    prev_lats = vessel_data['Latitude'].shift(1)\n",
    "    prev_lons = vessel_data['Longitude'].shift(1)\n",
    "    prev_times = vessel_data['Timestamp'].shift(1)\n",
    "    \n",
    "    # Calculate time differences in hours\n",
    "    time_diffs = (vessel_data['Timestamp'] - prev_times).dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate distances (vectorized)\n",
    "    distances = np.array([\n",
    "        calculate_distance(prev_lat, prev_lon, lat, lon) \n",
    "        if pd.notna(prev_lat) else 0\n",
    "        for prev_lat, prev_lon, lat, lon in zip(prev_lats, prev_lons, vessel_data['Latitude'], vessel_data['Longitude'])\n",
    "    ])\n",
    "\n",
    "    return distances, time_diffs\n",
    "\n",
    "def process_vessel_data(vessel_group):\n",
    "    \"\"\"Process a single vessel's data to calculate speeds and filter low-speed points.\"\"\"\n",
    "    mmsi = vessel_group.iloc[0]['MMSI']\n",
    "    \n",
    "    # Sort by timestamp and remove duplicates\n",
    "    vessel_data = vessel_group.sort_values('Timestamp').drop_duplicates('Timestamp').reset_index(drop=True)\n",
    "    \n",
    "    if len(vessel_data) < 2:\n",
    "        return pd.DataFrame()  # Not enough data points\n",
    "    \n",
    "    # Calculate distances and time differences\n",
    "    distances, time_diffs = include_spacetime_features(vessel_data)\n",
    "    \n",
    "    # Identify sudden jumps (zero time diff with significant distance change)\n",
    "    sudden_jumps = (time_diffs == 0) & (distances > SUDDEN_JUMP_THRESHOLD)\n",
    "    \n",
    "    # Calculate speeds (avoiding division by zero)\n",
    "    speeds = np.divide(distances, time_diffs, out=np.zeros_like(distances), where=time_diffs > 0)\n",
    "    vessel_data['speed_kmh'] = speeds\n",
    "    \n",
    "    # Remove sudden jumps and keep only low-speed points (potential port areas)\n",
    "    valid_points = (~sudden_jumps) & ((vessel_data['speed_kmh'] <= LOW_SPEED_THRESHOLD) | (vessel_data.index == 0))\n",
    "    low_speed_data = vessel_data[valid_points].copy()\n",
    "\n",
    "    if len(low_speed_data) > 0:\n",
    "        return low_speed_data[['MMSI', 'Timestamp', 'Latitude', 'Longitude', 'speed_kmh', 'Type of mobile']]\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_vessel_batches(vessel_data, time_window_hours=1/60, spatial_threshold_km=0.1):\n",
    "    \"\"\"Create batches for a single vessel from its low-speed data.\"\"\"\n",
    "    if len(vessel_data) < MINIMUM_POINTS_FOR_BATCH:\n",
    "        return []\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    distances, time_diffs = include_spacetime_features(vessel_data)\n",
    "    vessel_data[\"distance\"] = distances\n",
    "    vessel_data[\"time_diff\"] = time_diffs\n",
    "    mmsi = vessel_data.iloc[0]['MMSI']\n",
    "    \n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    \n",
    "    for idx, row in vessel_data.iterrows():\n",
    "        if not current_batch:\n",
    "            current_batch.append(row)\n",
    "        else:\n",
    "            # Check if current point should be added to current batch\n",
    "            time_diff = row['time_diff']\n",
    "            distance = row['distance']\n",
    "            if time_diff <= time_window_hours and distance <= spatial_threshold_km:\n",
    "                current_batch.append(row)\n",
    "            else:\n",
    "                # Save current batch if it has enough points\n",
    "                if len(current_batch) >= MINIMUM_POINTS_FOR_BATCH:\n",
    "                    batch_df = pd.DataFrame(current_batch)\n",
    "                    center_lat = batch_df['Latitude'].mean()\n",
    "                    center_lon = batch_df['Longitude'].mean()\n",
    "                    start_time = batch_df['Timestamp'].min()\n",
    "                    end_time = batch_df['Timestamp'].max()\n",
    "                    \n",
    "                    batches.append({\n",
    "                        'mmsi': mmsi,\n",
    "                        'center_lat': center_lat,\n",
    "                        'center_lon': center_lon,\n",
    "                        'start_time': start_time,\n",
    "                        'end_time': end_time,\n",
    "                        'point_count': len(current_batch),\n",
    "                        'mobile_type': current_batch[0]['Type of mobile']\n",
    "                    })\n",
    "                \n",
    "                # Start new batch\n",
    "                current_batch = [row]\n",
    "    \n",
    "    # Don't forget the last batch\n",
    "    if len(current_batch) >= MINIMUM_POINTS_FOR_BATCH:\n",
    "        batch_df = pd.DataFrame(current_batch)\n",
    "        center_lat = batch_df['Latitude'].mean()\n",
    "        center_lon = batch_df['Longitude'].mean()\n",
    "        start_time = batch_df['Timestamp'].min()\n",
    "        end_time = batch_df['Timestamp'].max()\n",
    "        \n",
    "        batches.append({\n",
    "            'mmsi': mmsi,\n",
    "            'center_lat': center_lat,\n",
    "            'center_lon': center_lon,\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'point_count': len(current_batch),\n",
    "            'mobile_type': current_batch[0]['Type of mobile']\n",
    "        })\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def load_filter_and_process_chunk(chunk):\n",
    "    \"\"\"Load, filter a chunk of data, process vessel data, and create batches\"\"\"\n",
    "    # Parse timestamp\n",
    "    chunk['Timestamp'] = pd.to_datetime(chunk['# Timestamp'])\n",
    "    \n",
    "    # Filter out Base Stations and N/A mobile units\n",
    "    chunk = chunk[chunk['Type of mobile'] != 'Base Station'].copy()\n",
    "    chunk = chunk.dropna(subset=['Type of mobile', 'MMSI', 'Latitude', 'Longitude'])\n",
    "    \n",
    "    # Remove invalid coordinates\n",
    "    chunk = chunk[\n",
    "        (chunk['Latitude'].between(-90, 90)) & \n",
    "        (chunk['Longitude'].between(-180, 180))\n",
    "    ]\n",
    "    \n",
    "    if chunk.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process each vessel in this chunk and create batches\n",
    "    all_batches = []\n",
    "    for mmsi, vessel_group in chunk.groupby('MMSI'):\n",
    "        # First get low-speed points for this vessel\n",
    "        vessel_low_speed = process_vessel_data(vessel_group)\n",
    "        \n",
    "        if not vessel_low_speed.empty:\n",
    "            # Then create batches from the low-speed points\n",
    "            vessel_batches = create_vessel_batches(vessel_low_speed)\n",
    "            all_batches.extend(vessel_batches)\n",
    "    \n",
    "    if all_batches:\n",
    "        return pd.DataFrame(all_batches)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d521689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 52/52 [02:24<00:00,  1.62s/chunk]"
     ]
    }
   ],
   "source": [
    "# Load, process data, and create batches in chunks with parallel processing\n",
    "\n",
    "chunk_size =   300000\n",
    "# chunk_size = 1000000 # optimal for lab2\n",
    "\n",
    "file_size = 15366011 # Size of the file lines\n",
    "tbar_total = file_size // chunk_size + 1\n",
    "tbar = tqdm(total=tbar_total, desc=\"Processing chunks\", unit=\"chunk\")\n",
    "\n",
    "# Create a generator of chunks\n",
    "chunks = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "all_results = []\n",
    "\n",
    "# Read chunks and process them in parallel\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    \n",
    "    future_to_chunk = {\n",
    "        executor.submit(load_filter_and_process_chunk, chunk): i for i, chunk in enumerate(chunks)\n",
    "    }\n",
    "    for future in as_completed(future_to_chunk):\n",
    "        chunk_index = future_to_chunk[future]\n",
    "        tbar.update(1)\n",
    "        try:\n",
    "            chunk_results = future.result()\n",
    "            all_results.append(chunk_results)\n",
    "            # print(f\"Processed chunk {chunk_index+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_index+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea261daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 34196 batches from 1075 vessels\n",
      "Average batch size: 123.3 points\n",
      "Batch duration range: 0.0 - 0.5 hours\n",
      "Unique vessels: 1075\n",
      "Mobile types in batches: mobile_type\n",
      "Class A    33261\n",
      "Class B      875\n",
      "AtoN          60\n",
      "Name: count, dtype: int64\n",
      "Total points in all batches: 4217775\n"
     ]
    }
   ],
   "source": [
    "# Concat all batch results into a single DataFrame\n",
    "batches_df = pd.concat(all_results, ignore_index=True)\n",
    "print(f\"Created {len(batches_df)} batches from {batches_df['mmsi'].nunique()} vessels\")\n",
    "\n",
    "print(f\"Average batch size: {batches_df['point_count'].mean():.1f} points\")\n",
    "print(f\"Batch duration range: {(batches_df['end_time'] - batches_df['start_time']).dt.total_seconds().min()/3600:.1f} - {(batches_df['end_time'] - batches_df['start_time']).dt.total_seconds().max()/3600:.1f} hours\")\n",
    "\n",
    "\n",
    "print(f\"Unique vessels: {batches_df['mmsi'].nunique()}\")\n",
    "print(f\"Mobile types in batches: {batches_df['mobile_type'].value_counts().head()}\")\n",
    "print(f\"Total points in all batches: {batches_df['point_count'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c7d13",
   "metadata": {},
   "source": [
    "## Create an algorithm for port detection.  \n",
    "\n",
    "- From the code above, we have a lot of batches. Each batch represents a vessel that has moved slowly in on place for a while. There might be multiple batches for the same vessel.\n",
    "- First, cluster the batches for each MMSI seperately - this way we save compute\n",
    "- If the vessel has been to the same location (allegedly port) multiple times, we will only keep one copy.\n",
    "- Then we do the clustering for all the ships, obtaining ports\n",
    "- Remove any ports with less than 10 unique MMSI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1d4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_batches(batches_df, min_samples, cluster_radius_km=1.0):\n",
    "    \"\"\"Cluster batches using DBSCAN\"\"\"\n",
    "    if len(batches_df) < 2:\n",
    "        print(\"Not enough batches for clustering\")\n",
    "        return []\n",
    "    \n",
    "    n = len(batches_df)\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = calculate_distance(\n",
    "                batches_df.iloc[i]['center_lat'], \n",
    "                batches_df.iloc[i]['center_lon'], \n",
    "                batches_df.iloc[j]['center_lat'], \n",
    "                batches_df.iloc[j]['center_lon']\n",
    "            )\n",
    "            distance_matrix[i, j] = distance_matrix[j, i] = dist\n",
    "\n",
    "    clustering = DBSCAN(eps=cluster_radius_km, min_samples=min_samples, metric=\"precomputed\").fit(\n",
    "        distance_matrix\n",
    "    )\n",
    "    return clustering.labels_.tolist()\n",
    "\n",
    "# Similar function to above, but using a more approximate method for clustering\n",
    "def cluster_batches_approx(batches_df, min_samples, cluster_radius_km=1.0):\n",
    "    \"\"\"Cluster batches using DBSCAN\"\"\"\n",
    "    if len(batches_df) < 2:\n",
    "        print(\"Not enough batches for clustering\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Prepare coordinates for clustering\n",
    "    coordinates = batches_df[['center_lat', 'center_lon']].values\n",
    "    \n",
    "    # Convert radius from km to degrees (approximate)\n",
    "    # 1 degree ≈ 111 km at equator\n",
    "    eps_degrees = cluster_radius_km / 111.0\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    clustering = DBSCAN(eps=eps_degrees, min_samples=min_samples).fit(coordinates)\n",
    "\n",
    "    return clustering.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407096a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_vessel_batches(group):\n",
    "    \"\"\"\n",
    "    For a single vessel, join all batches that were closed together into one.\n",
    "    Return the average center_lat, center_lon, min start_time, max end_time, and sum of point_count.\n",
    "    \"\"\"\n",
    "    return pd.Series({\n",
    "        'mmsi': group['mmsi'].iloc[0],\n",
    "        'center_lat': group['center_lat'].mean(),\n",
    "        'center_lon': group['center_lon'].mean(),\n",
    "        'start_time': group['start_time'].min(),\n",
    "        'end_time': group['end_time'].max(),\n",
    "        'point_count': group['point_count'].sum(),\n",
    "        'mobile_type': group['mobile_type'].mode()[0]  # Most common type\n",
    "    })\n",
    "\n",
    "def process_batches(chunk):\n",
    "    group = chunk[1]\n",
    "    if len(group) < 2:\n",
    "        return group\n",
    "    \n",
    "    # Cluster the batches for this vessel\n",
    "    labels = cluster_batches_approx(group, min_samples=1)\n",
    "    group['cluster'] = labels\n",
    "    group = group.groupby('cluster').apply(concat_vessel_batches).reset_index(drop=True)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887b764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "chunks = batches_df.groupby('mmsi')\n",
    "all_results = []\n",
    "\n",
    "t_bar = tqdm(total=len(chunks), desc=\"Processing batches\")\n",
    "\n",
    "# Read chunks and process them in parallel\n",
    "with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "    \n",
    "    future_to_chunk = {\n",
    "        executor.submit(process_batches, chunk): i for i, chunk in enumerate(chunks)\n",
    "    }\n",
    "    for future in as_completed(future_to_chunk):\n",
    "        t_bar.update(1)\n",
    "        chunk_index = future_to_chunk[future]\n",
    "        try:\n",
    "            chunk_results = future.result()\n",
    "            all_results.append(chunk_results)\n",
    "            # print(f\"Processed chunk {chunk_index+1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk {chunk_index+1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7db22d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered batches: 4143 from 34196 original batches\n",
      "Unique vessels after filtering: 1075\n",
      "Unique vessels before filtering: 1075\n"
     ]
    }
   ],
   "source": [
    "filtered_batches_df = pd.concat(all_results, ignore_index=True)\n",
    "print(f\"Filtered batches: {len(filtered_batches_df)} from {len(batches_df)} original batches\")\n",
    "# unique MMSI \n",
    "print(f\"Unique vessels after filtering: {filtered_batches_df['mmsi'].nunique()}\")\n",
    "# before filtering\n",
    "print(f\"Unique vessels before filtering: {batches_df['mmsi'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36516d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the filtered batches into ports\n",
    "port_clusters = cluster_batches_approx(filtered_batches_df, min_samples=10, cluster_radius_km=1.0)\n",
    "filtered_batches_df['cluster'] = port_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c646ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the cluster to -1 if it does not have more than 10 unique MMSI\n",
    "small_cluster = filtered_batches_df.groupby('cluster').apply(lambda x: x['mmsi'].nunique() < 10)\n",
    "filtered_batches_df.loc[filtered_batches_df['cluster'].isin(small_cluster[small_cluster].index), 'cluster'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cceaf150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total valid port batches: 487\n",
      "Vessels in ports: 406\n",
      "Ports with more than 10 vessels: 26\n"
     ]
    }
   ],
   "source": [
    "valid_ports = filtered_batches_df[filtered_batches_df['cluster'] != -1]\n",
    "print(f\"\\nTotal valid port batches: {len(valid_ports)}\")\n",
    "print(f\"Vessels in ports: {valid_ports['mmsi'].nunique()}\")\n",
    "print(f\"Ports with more than 10 vessels: {valid_ports['cluster'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72f8370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_batches = valid_ports.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849b8a22",
   "metadata": {},
   "source": [
    "## Evaluate the relative size of the port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53229de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PORT STATISTICS ===\n",
      "\n",
      "Port 0:\n",
      "  Location: (55.4662, 8.4098)\n",
      "  Size: 1.69 km × 1.38 km (Area: 2.34 km²)\n",
      "  Vessels: 27 unique MMSI\n",
      "  Batches: 27\n",
      "  Total points: 139413\n",
      "  Top mobile types: {'Class A': 27}\n",
      "\n",
      "Port 1:\n",
      "  Location: (54.6360, 11.3739)\n",
      "  Size: 0.70 km × 0.81 km (Area: 0.56 km²)\n",
      "  Vessels: 12 unique MMSI\n",
      "  Batches: 12\n",
      "  Total points: 42070\n",
      "  Top mobile types: {'Class A': 12}\n",
      "\n",
      "Port 2:\n",
      "  Location: (54.2580, 9.6214)\n",
      "  Size: 0.91 km × 0.36 km (Area: 0.33 km²)\n",
      "  Vessels: 11 unique MMSI\n",
      "  Batches: 11\n",
      "  Total points: 827\n",
      "  Top mobile types: {'Class A': 11}\n",
      "\n",
      "Port 3:\n",
      "  Location: (55.3630, 13.1470)\n",
      "  Size: 0.83 km × 0.94 km (Area: 0.78 km²)\n",
      "  Vessels: 10 unique MMSI\n",
      "  Batches: 12\n",
      "  Total points: 2099\n",
      "  Top mobile types: {'Class A': 12}\n",
      "\n",
      "Port 4:\n",
      "  Location: (54.6525, 11.3478)\n",
      "  Size: 0.50 km × 0.17 km (Area: 0.09 km²)\n",
      "  Vessels: 14 unique MMSI\n",
      "  Batches: 14\n",
      "  Total points: 13668\n",
      "  Top mobile types: {'Class A': 14}\n",
      "\n",
      "Port 5:\n",
      "  Location: (55.0873, 14.6776)\n",
      "  Size: 1.92 km × 1.19 km (Area: 2.29 km²)\n",
      "  Vessels: 16 unique MMSI\n",
      "  Batches: 19\n",
      "  Total points: 51757\n",
      "  Top mobile types: {'Class A': 16, 'Class B': 3}\n",
      "\n",
      "Port 6:\n",
      "  Location: (54.1462, 12.0976)\n",
      "  Size: 1.55 km × 0.98 km (Area: 1.52 km²)\n",
      "  Vessels: 12 unique MMSI\n",
      "  Batches: 14\n",
      "  Total points: 1017\n",
      "  Top mobile types: {'Class A': 14}\n",
      "\n",
      "Port 7:\n",
      "  Location: (57.7138, 10.5814)\n",
      "  Size: 0.94 km × 1.36 km (Area: 1.29 km²)\n",
      "  Vessels: 43 unique MMSI\n",
      "  Batches: 48\n",
      "  Total points: 255614\n",
      "  Top mobile types: {'Class A': 47, 'Class B': 1}\n",
      "\n",
      "Port 8:\n",
      "  Location: (56.6929, 8.2187)\n",
      "  Size: 1.40 km × 0.36 km (Area: 0.51 km²)\n",
      "  Vessels: 18 unique MMSI\n",
      "  Batches: 18\n",
      "  Total points: 102529\n",
      "  Top mobile types: {'Class A': 17, 'Class B': 1}\n",
      "\n",
      "Port 9:\n",
      "  Location: (57.1170, 8.5902)\n",
      "  Size: 0.92 km × 0.75 km (Area: 0.68 km²)\n",
      "  Vessels: 35 unique MMSI\n",
      "  Batches: 35\n",
      "  Total points: 244747\n",
      "  Top mobile types: {'Class A': 34, 'Class B': 1}\n",
      "\n",
      "Port 29:\n",
      "  Location: (57.6862, 11.8647)\n",
      "  Size: 0.59 km × 0.73 km (Area: 0.43 km²)\n",
      "  Vessels: 10 unique MMSI\n",
      "  Batches: 10\n",
      "  Total points: 650\n",
      "  Top mobile types: {'Class A': 10}\n",
      "\n",
      "Port 12:\n",
      "  Location: (57.4329, 10.5360)\n",
      "  Size: 1.10 km × 1.07 km (Area: 1.18 km²)\n",
      "  Vessels: 15 unique MMSI\n",
      "  Batches: 15\n",
      "  Total points: 75847\n",
      "  Top mobile types: {'Class A': 15}\n",
      "\n",
      "Port 10:\n",
      "  Location: (55.4537, 8.4356)\n",
      "  Size: 1.16 km × 1.00 km (Area: 1.16 km²)\n",
      "  Vessels: 14 unique MMSI\n",
      "  Batches: 14\n",
      "  Total points: 37909\n",
      "  Top mobile types: {'Class A': 14}\n",
      "\n",
      "Port 11:\n",
      "  Location: (55.0573, 10.6150)\n",
      "  Size: 0.43 km × 0.40 km (Area: 0.17 km²)\n",
      "  Vessels: 11 unique MMSI\n",
      "  Batches: 11\n",
      "  Total points: 47797\n",
      "  Top mobile types: {'Class A': 10, 'Class B': 1}\n",
      "\n",
      "Port 13:\n",
      "  Location: (57.3205, 11.1235)\n",
      "  Size: 0.17 km × 0.28 km (Area: 0.05 km²)\n",
      "  Vessels: 18 unique MMSI\n",
      "  Batches: 18\n",
      "  Total points: 108488\n",
      "  Top mobile types: {'Class A': 17, 'Class B': 1}\n",
      "\n",
      "Port 14:\n",
      "  Location: (57.4918, 10.5014)\n",
      "  Size: 0.52 km × 0.30 km (Area: 0.16 km²)\n",
      "  Vessels: 18 unique MMSI\n",
      "  Batches: 18\n",
      "  Total points: 90541\n",
      "  Top mobile types: {'Class A': 18}\n",
      "\n",
      "Port 15:\n",
      "  Location: (55.9985, 8.1188)\n",
      "  Size: 1.11 km × 1.03 km (Area: 1.14 km²)\n",
      "  Vessels: 17 unique MMSI\n",
      "  Batches: 17\n",
      "  Total points: 94451\n",
      "  Top mobile types: {'Class A': 14, 'Class B': 3}\n",
      "\n",
      "Port 16:\n",
      "  Location: (55.6880, 12.6200)\n",
      "  Size: 1.22 km × 0.70 km (Area: 0.85 km²)\n",
      "  Vessels: 11 unique MMSI\n",
      "  Batches: 12\n",
      "  Total points: 37992\n",
      "  Top mobile types: {'Class A': 11, 'Class B': 1}\n",
      "\n",
      "Port 17:\n",
      "  Location: (56.1260, 12.3088)\n",
      "  Size: 0.23 km × 0.16 km (Area: 0.04 km²)\n",
      "  Vessels: 15 unique MMSI\n",
      "  Batches: 15\n",
      "  Total points: 96632\n",
      "  Top mobile types: {'Class A': 15}\n",
      "\n",
      "Port 18:\n",
      "  Location: (57.5895, 9.9538)\n",
      "  Size: 1.53 km × 1.57 km (Area: 2.40 km²)\n",
      "  Vessels: 51 unique MMSI\n",
      "  Batches: 55\n",
      "  Total points: 254426\n",
      "  Top mobile types: {'Class A': 49, 'Class B': 6}\n",
      "\n",
      "Port 21:\n",
      "  Location: (58.0024, 10.2842)\n",
      "  Size: 1.29 km × 0.95 km (Area: 1.22 km²)\n",
      "  Vessels: 10 unique MMSI\n",
      "  Batches: 11\n",
      "  Total points: 1092\n",
      "  Top mobile types: {'Class A': 10, 'Class B': 1}\n",
      "\n",
      "Port 19:\n",
      "  Location: (56.4083, 10.9218)\n",
      "  Size: 1.33 km × 0.79 km (Area: 1.05 km²)\n",
      "  Vessels: 20 unique MMSI\n",
      "  Batches: 20\n",
      "  Total points: 86274\n",
      "  Top mobile types: {'Class A': 18, 'Class B': 2}\n",
      "\n",
      "Port 22:\n",
      "  Location: (57.8962, 9.6904)\n",
      "  Size: 1.46 km × 1.09 km (Area: 1.59 km²)\n",
      "  Vessels: 10 unique MMSI\n",
      "  Batches: 12\n",
      "  Total points: 969\n",
      "  Top mobile types: {'Class A': 12}\n",
      "\n",
      "Port 23:\n",
      "  Location: (57.9176, 9.7803)\n",
      "  Size: 2.76 km × 1.63 km (Area: 4.48 km²)\n",
      "  Vessels: 16 unique MMSI\n",
      "  Batches: 25\n",
      "  Total points: 1176\n",
      "  Top mobile types: {'Class A': 25}\n",
      "\n",
      "Port 25:\n",
      "  Location: (57.9341, 9.8152)\n",
      "  Size: 1.70 km × 0.95 km (Area: 1.61 km²)\n",
      "  Vessels: 10 unique MMSI\n",
      "  Batches: 12\n",
      "  Total points: 809\n",
      "  Top mobile types: {'Class A': 12}\n",
      "\n",
      "Port 27:\n",
      "  Location: (55.4476, 8.4644)\n",
      "  Size: 0.44 km × 1.13 km (Area: 0.50 km²)\n",
      "  Vessels: 11 unique MMSI\n",
      "  Batches: 12\n",
      "  Total points: 20747\n",
      "  Top mobile types: {'Class A': 12}\n"
     ]
    }
   ],
   "source": [
    "def calculate_port_stats(clustered_batches):\n",
    "    \"\"\"Calculate statistics for each detected port.\"\"\"\n",
    "    port_stats = []\n",
    "    \n",
    "    for cluster_id in clustered_batches['cluster'].unique():\n",
    "        \n",
    "        cluster_data = clustered_batches[clustered_batches['cluster'] == cluster_id]\n",
    "        \n",
    "        # Calculate bounding box\n",
    "        min_lat = cluster_data['center_lat'].min()\n",
    "        max_lat = cluster_data['center_lat'].max()\n",
    "        min_lon = cluster_data['center_lon'].min()\n",
    "        max_lon = cluster_data['center_lon'].max()\n",
    "        \n",
    "        # Calculate grid size in km\n",
    "        lat_size_km = calculate_distance(min_lat, min_lon, max_lat, min_lon)\n",
    "        lon_size_km = calculate_distance(min_lat, min_lon, min_lat, max_lon)\n",
    "        area_km2 = lat_size_km * lon_size_km\n",
    "        \n",
    "        # Calculate center point\n",
    "        center_lat = cluster_data['center_lat'].mean()\n",
    "        center_lon = cluster_data['center_lon'].mean()\n",
    "        \n",
    "        # Other statistics\n",
    "        total_batches = len(cluster_data)\n",
    "        unique_mmsi = cluster_data['mmsi'].nunique()\n",
    "        total_points = cluster_data['point_count'].sum()\n",
    "        mobile_types = cluster_data['mobile_type'].value_counts().to_dict()\n",
    "        \n",
    "        port_stats.append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'min_lat': min_lat,\n",
    "            'max_lat': max_lat,\n",
    "            'min_lon': min_lon,\n",
    "            'max_lon': max_lon,\n",
    "            'lat_size_km': lat_size_km,\n",
    "            'lon_size_km': lon_size_km,\n",
    "            'area_km2': area_km2,\n",
    "            'total_batches': total_batches,\n",
    "            'unique_mmsi': unique_mmsi,\n",
    "            'total_points': total_points,\n",
    "            'mobile_types': mobile_types\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(port_stats)\n",
    "\n",
    "port_stats_df = calculate_port_stats(clustered_batches)\n",
    "\n",
    "print(f\"\\n=== PORT STATISTICS ===\")\n",
    "for _, port in port_stats_df.iterrows():\n",
    "    print(f\"\\nPort {port['cluster_id']}:\")\n",
    "    print(f\"  Location: ({port['min_lat']:.4f}, {port['min_lon']:.4f})\")\n",
    "    print(f\"  Size: {port['lat_size_km']:.2f} km × {port['lon_size_km']:.2f} km (Area: {port['area_km2']:.2f} km²)\")\n",
    "    print(f\"  Vessels: {port['unique_mmsi']} unique MMSI\")\n",
    "    print(f\"  Batches: {port['total_batches']}\")\n",
    "    print(f\"  Total points: {port['total_points']}\")\n",
    "    # Show top 3 mobile types\n",
    "    top_types = dict(list(port['mobile_types'].items())[:3])\n",
    "    print(f\"  Top mobile types: {top_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2125808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Port ID</th>\n",
       "      <th>Area (km²)</th>\n",
       "      <th>Unique Vessels</th>\n",
       "      <th>Total Batches</th>\n",
       "      <th>Data Points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>4.484977</td>\n",
       "      <td>16</td>\n",
       "      <td>25</td>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>2.395893</td>\n",
       "      <td>51</td>\n",
       "      <td>55</td>\n",
       "      <td>254426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.341357</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>139413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.086667</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>13668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.045768</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>108488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>0.035793</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>96632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Port ID  Area (km²)  Unique Vessels  Total Batches  Data Points\n",
       "0       23    4.484977              16             25         1176\n",
       "1       18    2.395893              51             55       254426\n",
       "2        0    2.341357              27             27       139413\n",
       "3        4    0.086667              14             14        13668\n",
       "4       13    0.045768              18             18       108488\n",
       "5       17    0.035793              15             15        96632"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_stat = port_stats_df[[\"cluster_id\", \"area_km2\", \"unique_mmsi\",  \"total_batches\",\"total_points\"]].sort_values(by=\"area_km2\", ascending=False)\n",
    "\n",
    "pd.concat([port_stat.head(3), port_stat.tail(3)], ignore_index=True).rename(columns={\n",
    "    \"cluster_id\": \"Port ID\",\n",
    "    \"area_km2\": \"Area (km²)\",\n",
    "    \"total_batches\": \"Total Batches\",\n",
    "    \"unique_mmsi\": \"Unique Vessels\",\n",
    "    \"total_points\": \"Data Points\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd210f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PORT DETECTION SUMMARY ===\n",
      "Total ports detected: 26\n",
      "Total vessels involved: 406\n",
      "Total low-speed batches in ports: 487\n",
      "\n",
      "Largest port: Port 23 (4.48 km²)\n",
      "Busiest port: Port 18 (51 vessels)\n"
     ]
    }
   ],
   "source": [
    "# Display summary\n",
    "print(f\"\\n=== PORT DETECTION SUMMARY ===\")\n",
    "print(f\"Total ports detected: {len(port_stats_df)}\")\n",
    "print(f\"Total vessels involved: {clustered_batches[clustered_batches['cluster'] != -1]['mmsi'].nunique()}\")\n",
    "print(f\"Total low-speed batches in ports: {len(clustered_batches[clustered_batches['cluster'] != -1])}\")\n",
    "\n",
    "# Show largest ports\n",
    "largest_port = port_stats_df.loc[port_stats_df['area_km2'].idxmax()]\n",
    "busiest_port = port_stats_df.loc[port_stats_df['unique_mmsi'].idxmax()]\n",
    "\n",
    "print(f\"\\nLargest port: Port {largest_port['cluster_id']} ({largest_port['area_km2']:.2f} km²)\")\n",
    "print(f\"Busiest port: Port {busiest_port['cluster_id']} ({busiest_port['unique_mmsi']} vessels)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4450ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notable ports:\n",
    "# 23 - largest\n",
    "# 18 - most vessels\n",
    "# 17 - smallest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668da95",
   "metadata": {},
   "source": [
    "## Visualize ports. (Get creative with it.) It's good to know the location of the port, its relative size, etc.\n",
    "\n",
    "- Use folium to place a mesh on clusters. Include their size, number of points and unique MMSI count.\n",
    "- Also mark each batch in the cluster, we can see that they sometimes represent the ship docking locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a90a429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_port_map(port_stats_df, clustered_batches):\n",
    "    \"\"\"Create an interactive map showing detected ports.\"\"\"\n",
    "    if port_stats_df.empty:\n",
    "        print(\"No ports to visualize\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate map starting pos, start the the first port\n",
    "    center_lat = port_stats_df['min_lat'].iloc[0]\n",
    "    center_lon = port_stats_df['min_lon'].iloc[0]\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=8)\n",
    "    \n",
    "    # Color palette for different ports\n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', \n",
    "             'beige', 'darkblue', 'darkgreen', 'cadetblue', 'darkpurple', 'white', \n",
    "             'pink', 'lightblue', 'lightgreen', 'gray', 'black', 'lightgray']\n",
    "    \n",
    "    # Add port areas\n",
    "    for idx, port in port_stats_df.iterrows():\n",
    "        color = colors[idx % len(colors)]\n",
    "        \n",
    "        # Create bounding box\n",
    "        bounds = [\n",
    "            [port['min_lat'], port['min_lon']],\n",
    "            [port['min_lat'], port['max_lon']],\n",
    "            [port['max_lat'], port['max_lon']],\n",
    "            [port['max_lat'], port['min_lon']]\n",
    "        ]\n",
    "        if port['cluster_id'] == 17:\n",
    "            color = \"red\"\n",
    "        # Add port boundary\n",
    "        folium.Polygon(\n",
    "            locations=bounds,\n",
    "            color=color,\n",
    "            weight=3,\n",
    "            fill=True,\n",
    "            fillOpacity=0.2,\n",
    "            popup=folium.Popup(\n",
    "                f\"\"\"<b>Port {port['cluster_id']}</b><br/>\n",
    "                Size: {port['lat_size_km']:.2f} × {port['lon_size_km']:.2f} km<br/>\n",
    "                Area: {port['area_km2']:.2f} km²<br/>\n",
    "                Vessels: {port['unique_mmsi']}<br/>\n",
    "                Batches: {port['total_batches']}<br/>\n",
    "                Points: {port['total_points']}\"\"\",\n",
    "                max_width=300\n",
    "            )\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add individual batch points\n",
    "        port_batches = clustered_batches[clustered_batches['cluster'] == port['cluster_id']]\n",
    "        for _, batch in port_batches.iterrows():\n",
    "            folium.CircleMarker(\n",
    "                location=[batch['center_lat'], batch['center_lon']],\n",
    "                radius=10,\n",
    "                popup=f\"MMSI: {batch['mmsi']}<br/>Points: {batch['point_count']}<br>Port {port['cluster_id']}\",\n",
    "                color=color,\n",
    "                fill=True,\n",
    "                fillOpacity=0.6,\n",
    "                weight=1\n",
    "            ).add_to(m)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215949aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved as 'detected_ports.html'\n"
     ]
    }
   ],
   "source": [
    "port_map = create_port_map(port_stats_df, clustered_batches)\n",
    "\n",
    "# Save the map\n",
    "port_map.save('detected_ports.html')\n",
    "print(\"Map saved as 'detected_ports.html'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata-3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
