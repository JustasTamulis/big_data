{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b019b26",
   "metadata": {},
   "source": [
    "## Example  - Simple Station Distance Calculation\n",
    "\n",
    "A clean, simple example that simulates your bike rental scenario:\n",
    "- **Reader**: Loads data files and creates station lookup table\n",
    "- **Workers**: Calculate distances between stations with caching\n",
    "- **Single lock**: Simple approach for shared data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aac1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "import time\n",
    "\n",
    "def data_reader(queue, lookup_table, lock, num_pairs=30, total_files=3):\n",
    "    \"\"\"Simulates loading bike rental data files\"\"\"\n",
    "    print(\"Reader: Starting...\")\n",
    "    \n",
    "    for file_num in range(1, total_files + 1):\n",
    "        print(f\"Reader: Processing file {file_num}/{total_files}\")\n",
    "        \n",
    "        # Add new stations to lookup table\n",
    "        with lock:\n",
    "            for station_id in range(1, 5 + file_num + 1):  \n",
    "                if station_id not in lookup_table:\n",
    "                    # Simulate station coordinates\n",
    "                    lookup_table[station_id] = random.uniform(10.0, 100.0)\n",
    "        \n",
    "        # Generate trip pairs (start_station, end_station)\n",
    "        for i in range(num_pairs):\n",
    "            start_station = random.randint(1, 5 + file_num)\n",
    "            end_station = random.randint(1, 5 + file_num)\n",
    "            if start_station != end_station:\n",
    "                queue.put((start_station, end_station))\n",
    "            time.sleep(0.1)  # simulate file reading\n",
    "        \n",
    "        time.sleep(0.5)  # simulate file processing\n",
    "    \n",
    "    # Signal workers to stop\n",
    "    queue.put(None)\n",
    "    print(\"Reader: Finished\")\n",
    "\n",
    "def distance_worker(worker_id, queue, lookup_table, distance_cache, results, lock):\n",
    "    \"\"\"Calculates distances between stations\"\"\"\n",
    "    print(f\"Worker {worker_id}: Starting...\")\n",
    "    processed = 0\n",
    "    calculated = 0\n",
    "    \n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        \n",
    "        if item is None:\n",
    "            queue.put(None)  # pass shutdown signal to other workers\n",
    "            break\n",
    "        \n",
    "        start_station, end_station = item\n",
    "        cache_key = (start_station, end_station)  # directional: (1,2) != (2,1)\n",
    "        \n",
    "        # Check cache first\n",
    "        with lock:\n",
    "            if cache_key in distance_cache:\n",
    "                distance = distance_cache[cache_key]\n",
    "                processed += 1\n",
    "                continue\n",
    "        # Calculate new distance\n",
    "            else:\n",
    "                coord1 = lookup_table[start_station]\n",
    "                coord2 = lookup_table[end_station]\n",
    "        \n",
    "        # Simulate slow distance calculation (like OSM routing)\n",
    "        time.sleep(0.5)\n",
    "        distance = abs(coord1 - coord2)  # simple distance\n",
    "        \n",
    "        # Save to cache and results\n",
    "        with lock:\n",
    "            distance_cache[cache_key] = distance\n",
    "\n",
    "        processed += 1\n",
    "        calculated += 1\n",
    "    \n",
    "    with lock:\n",
    "        results.append(\n",
    "            {\"worker_id\": worker_id, \"processed\": processed, \"calculated\": calculated}\n",
    "        )\n",
    "\n",
    "    print(f\"Worker {worker_id}: Finished with {processed} processed, {calculated} calculated distances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "899618f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 reader and 3 workers...\n",
      "Reader: Starting...\n",
      "Reader: Processing file 1/3\n",
      "Worker 0: Starting...\n",
      "Worker 1: Starting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 2: Starting...\n",
      "Reader: Processing file 2/3\n",
      "Reader: Processing file 3/3\n",
      "Reader: FinishedWorker 2: Finished with 19 processed, 10 calculated distancesWorker 1: Finished with 15 processed, 11 calculated distances\n",
      "\n",
      "Worker 0: Finished with 17 processed, 11 calculated distances\n",
      "\n",
      "\n",
      "✅ Completed in 7.70 seconds\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Setup shared data structures\n",
    "manager = mp.Manager()\n",
    "queue = manager.Queue()\n",
    "lookup_table = manager.dict()  # station_id -> coordinate\n",
    "distance_cache = manager.dict()  # (start_station, end_station) -> distance\n",
    "results = manager.list()  # trip results\n",
    "lock = manager.Lock()  # single lock for all shared data\n",
    "\n",
    "# Configuration\n",
    "num_workers = 3\n",
    "total_files = 3\n",
    "\n",
    "# Create processes\n",
    "reader_process = mp.Process(\n",
    "    target=data_reader,\n",
    "    args=(queue, lookup_table, lock, 20, total_files)\n",
    ")\n",
    "\n",
    "worker_processes = [\n",
    "    mp.Process(\n",
    "        target=distance_worker,\n",
    "        args=(i, queue, lookup_table, distance_cache, results, lock)\n",
    "    )\n",
    "    for i in range(num_workers)\n",
    "]\n",
    "\n",
    "print(f\"Starting 1 reader and {num_workers} workers...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Start all processes\n",
    "reader_process.start()\n",
    "for worker in worker_processes:\n",
    "    worker.start()\n",
    "\n",
    "# Wait for completion\n",
    "reader_process.join()\n",
    "for worker in worker_processes:\n",
    "    worker.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n✅ Completed in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d3c7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker_id</th>\n",
       "      <th>processed</th>\n",
       "      <th>calculated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker_id  processed  calculated\n",
       "0          2         19          10\n",
       "1          1         15          11\n",
       "2          0         17          11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "workers_results = pd.DataFrame(list(results))\n",
    "workers_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37559366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Results Summary:\n",
      "Stations in lookup table: 8\n",
      "Cached routes: 30\n",
      "Total items processed: 51\n",
      "Calculated (new): 32\n",
      "Cached (reused): 19\n",
      "Cache hit rate: 37.3%\n",
      "\n",
      "👥 Worker Performance:\n",
      "  Worker 2: 19 total (10 calculated, 9 cached)\n",
      "  Worker 1: 15 total (11 calculated, 4 cached)\n",
      "  Worker 0: 17 total (11 calculated, 6 cached)\n",
      "\n",
      "🗂️ Sample cached routes (first 8):\n",
      "  Route 2→4: Distance = 13.14\n",
      "  Route 1→5: Distance = 55.21\n",
      "  Route 4→3: Distance = 50.38\n",
      "  Route 5→1: Distance = 55.21\n",
      "  Route 3→1: Distance = 12.87\n",
      "  Route 1→2: Distance = 50.65\n",
      "  Route 4→2: Distance = 13.14\n",
      "  Route 5→4: Distance = 17.70\n"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "print(f\"\\n📊 Results Summary:\")\n",
    "print(f\"Stations in lookup table: {len(lookup_table)}\")\n",
    "print(f\"Cached routes: {len(distance_cache)}\")\n",
    "\n",
    "# Worker statistics from the new format\n",
    "total_processed = workers_results['processed'].sum()\n",
    "total_calculated = workers_results['calculated'].sum()\n",
    "cached_count = total_processed - total_calculated\n",
    "\n",
    "if total_processed > 0:\n",
    "    print(f\"Total items processed: {total_processed}\")\n",
    "    print(f\"Calculated (new): {total_calculated}\")\n",
    "    print(f\"Cached (reused): {cached_count}\")\n",
    "    print(f\"Cache hit rate: {cached_count/total_processed*100:.1f}%\")\n",
    "\n",
    "# Show worker performance\n",
    "print(f\"\\n👥 Worker Performance:\")\n",
    "for result in workers_results.to_dict('records'):\n",
    "    worker_id = result['worker_id']\n",
    "    processed = result['processed']\n",
    "    calculated = result['calculated']\n",
    "    cached = processed - calculated\n",
    "    print(f\"  Worker {worker_id}: {processed} total ({calculated} calculated, {cached} cached)\")\n",
    "\n",
    "# Show sample cache entries\n",
    "print(f\"\\n🗂️ Sample cached routes (first 8):\")\n",
    "for i, (route, distance) in enumerate(list(distance_cache.items())[:8]):\n",
    "    start, end = route\n",
    "    print(f\"  Route {start}→{end}: Distance = {distance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753b6460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Station 1: Coordinate 74.77\n",
      "  Station 2: Coordinate 24.12\n",
      "  Station 3: Coordinate 87.64\n",
      "  Station 4: Coordinate 37.26\n",
      "  Station 5: Coordinate 19.56\n",
      "  Station 6: Coordinate 98.48\n",
      "  Station 7: Coordinate 95.07\n",
      "  Station 8: Coordinate 28.38\n"
     ]
    }
   ],
   "source": [
    "for station_id, coord in sorted(lookup_table.items()):\n",
    "    print(f\"  Station {station_id}: Coordinate {coord:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2301c44b",
   "metadata": {},
   "source": [
    "## Example of a process with dashbaord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "745e02dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files 1/3 | Queue 20 | Waiting 0/3 | Processed 0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files 3/3 | Queue 1 | Waiting 0/3 | Processed 597\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import time, random, math\n",
    "from queue import Empty\n",
    "\n",
    "# ── shared counters ──────────────────────────────\n",
    "files_done      = mp.Value('i', 0)   # files processed\n",
    "items_in_q      = mp.Value('i', 0)   # queue length\n",
    "items_done      = mp.Value('i', 0)   # total pairs processed\n",
    "workers_waiting = mp.Value('i', 0)   # idle workers\n",
    "\n",
    "def inc(var, n=1):\n",
    "    with var.get_lock():\n",
    "        var.value += n\n",
    "\n",
    "# ── producer ─────────────────────────────────────\n",
    "def reader(q, total_files=3, pairs_per_file=20):\n",
    "    for _ in range(total_files):\n",
    "        for _ in range(pairs_per_file):\n",
    "            q.put((random.randint(1, 99), random.randint(1, 99)))\n",
    "            inc(items_in_q)\n",
    "        inc(files_done)\n",
    "        time.sleep(0.5)              # simulate file load\n",
    "    q.put((None, None))              # poison pill\n",
    "\n",
    "# ── consumer ─────────────────────────────────────\n",
    "def worker(wid, q):\n",
    "    while True:\n",
    "        try:\n",
    "            inc(workers_waiting)     # about to block\n",
    "            a, b = q.get(timeout=1)\n",
    "            inc(workers_waiting, -1)\n",
    "        except Empty:\n",
    "            inc(workers_waiting, -1)\n",
    "            continue\n",
    "        if a is None:\n",
    "            q.put((None, None))      # pass poison pill\n",
    "            break\n",
    "        math.gcd(a, b)               # dummy work\n",
    "        time.sleep(0.1)\n",
    "        inc(items_in_q, -1)\n",
    "        inc(items_done)\n",
    "\n",
    "# ── tiny dashboard (parent process) ─────────────\n",
    "def monitor(total_files, nworkers, procs):\n",
    "    while any(p.is_alive() for p in procs):\n",
    "        print(f\"\\rFiles {files_done.value}/{total_files} | \"\n",
    "              f\"Queue {items_in_q.value} | \"\n",
    "              f\"Waiting {workers_waiting.value}/{nworkers} | \"\n",
    "              f\"Processed {items_done.value}\", end=\"\", flush=True)\n",
    "        time.sleep(0.2)\n",
    "    print()  # newline when done\n",
    "\n",
    "# ── bootstrap ───────────────────────────────────\n",
    "if __name__ == '__main__':\n",
    "    total_files, nworkers = 3, 3\n",
    "    q = mp.Queue()\n",
    "\n",
    "    procs = [mp.Process(target=reader, args=(q, total_files))] + \\\n",
    "            [mp.Process(target=worker, args=(i, q)) for i in range(nworkers)]\n",
    "\n",
    "    for p in procs:\n",
    "        p.start()\n",
    "\n",
    "    monitor(total_files, nworkers, procs)\n",
    "\n",
    "    for p in procs:\n",
    "        p.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bcdc60",
   "metadata": {},
   "source": [
    "## Bike trip with dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53a26a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compact bike-trip example with a live one-line dashboard.\n",
    "\n",
    "‣ One reader process:\n",
    "    – adds new “station” coordinates,\n",
    "    – emits <start,end> pairs for each fake CSV file.\n",
    "\n",
    "‣ N worker processes:\n",
    "    – pull pairs, look up / cache a distance, simulate a slow call.\n",
    "\n",
    "Shared counters give the dashboard:\n",
    "    Files 1/3 | Queue 14 | Waiting 2/3 | Processed 26\n",
    "\"\"\"\n",
    "\n",
    "import multiprocessing as mp, time, random, math\n",
    "from queue import Empty          # non-blocking Queue ops\n",
    "\n",
    "# ───────────────────────── helpers ──────────────────────────────\n",
    "def inc(val, n=1):               # atomic += n for a mp.Value\n",
    "    with val.get_lock():\n",
    "        val.value += n\n",
    "\n",
    "# ──────────────────────── producer ──────────────────────────────\n",
    "def data_reader(queue,\n",
    "                lookup_table, lock,\n",
    "                total_files, pairs_per_file,\n",
    "                items_in_q, files_done):\n",
    "    for f in range(1, total_files + 1):\n",
    "        # new stations for this “file”\n",
    "        with lock:\n",
    "            for sid in range(1, 5 + f + 1):\n",
    "                lookup_table.setdefault(sid, random.uniform(10.0, 100.0))\n",
    "\n",
    "        # emit trip pairs\n",
    "        for _ in range(pairs_per_file):\n",
    "            a, b = random.randint(1, 5 + f), random.randint(1, 5 + f)\n",
    "            if a == b:\n",
    "                continue\n",
    "            queue.put((a, b))\n",
    "            inc(items_in_q)\n",
    "            time.sleep(0.3)          # simulate reading delay\n",
    "\n",
    "        inc(files_done)              # one file done\n",
    "        time.sleep(1.5)              # simulate file processing\n",
    "\n",
    "    queue.put((None, None))          # poison pill\n",
    "\n",
    "# ──────────────────────── consumer ─────────────────────────────\n",
    "def distance_worker(wid, queue,\n",
    "                    lookup_table, distance_cache, results, lock,\n",
    "                    items_in_q, items_done, workers_waiting):\n",
    "    processed = calculated = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            inc(workers_waiting)     # going to block\n",
    "            a, b = queue.get(timeout=1)\n",
    "            inc(workers_waiting, -1)\n",
    "        except Empty:                # nothing to do\n",
    "            inc(workers_waiting, -1)\n",
    "            continue\n",
    "\n",
    "        if a is None:                # shutdown signal\n",
    "            queue.put((None, None))\n",
    "            break\n",
    "\n",
    "        key = (a, b)\n",
    "        with lock:                   # cache lookup / insert\n",
    "            dist = distance_cache.get(key)\n",
    "            if dist is None:\n",
    "                dist = abs(lookup_table[a] - lookup_table[b])\n",
    "                distance_cache[key] = dist\n",
    "                calculated += 1\n",
    "\n",
    "        time.sleep(0.5)              # simulate “slow” call\n",
    "        processed += 1\n",
    "        inc(items_in_q, -1)\n",
    "        inc(items_done)\n",
    "\n",
    "    with lock:\n",
    "        results.append(dict(worker=wid,\n",
    "                            processed=processed,\n",
    "                            calculated=calculated))\n",
    "\n",
    "# ────────────────── tiny dashboard in parent ───────────────────\n",
    "def monitor(total_files, nworkers,\n",
    "            files_done, items_in_q, workers_waiting, items_done,\n",
    "            procs):\n",
    "    while any(p.is_alive() for p in procs):\n",
    "        print(f\"\\rFiles {files_done.value}/{total_files} | \"\n",
    "              f\"Queue {items_in_q.value} | \"\n",
    "              f\"Waiting {workers_waiting.value}/{nworkers} | \"\n",
    "              f\"Processed {items_done.value}\",\n",
    "              end=\"\", flush=True)\n",
    "        time.sleep(0.2)\n",
    "    print()                          # newline when finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d9940c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files 3/3 | Queue 0 | Waiting 0/3 | Processed 55\n",
      "\n",
      "Per-worker stats: [{'worker': 2, 'processed': 18, 'calculated': 9}, {'worker': 1, 'processed': 18, 'calculated': 10}, {'worker': 0, 'processed': 19, 'calculated': 11}]\n",
      "Cached distances : 30\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────── bootstrap ────────────────────────────\n",
    "# mp.set_start_method(\"spawn\")     # safe on all OSes\n",
    "\n",
    "TOTAL_FILES, PAIRS_PER_FILE = 3, 20\n",
    "NWORKERS = 3\n",
    "\n",
    "queue = mp.Queue()\n",
    "manager = mp.Manager()\n",
    "lookup_table   = manager.dict()      # station_id → coord\n",
    "distance_cache = manager.dict()      # (a,b)     → dist\n",
    "results        = manager.list()\n",
    "lock           = manager.Lock()\n",
    "\n",
    "# shared dashboard counters\n",
    "files_done      = mp.Value('i', 0)\n",
    "items_in_q      = mp.Value('i', 0)\n",
    "items_done      = mp.Value('i', 0)\n",
    "workers_waiting = mp.Value('i', 0)\n",
    "\n",
    "reader = mp.Process(target=data_reader,\n",
    "                  args=(queue, lookup_table, lock,\n",
    "                        TOTAL_FILES, PAIRS_PER_FILE,\n",
    "                        items_in_q, files_done))\n",
    "\n",
    "workers = [mp.Process(target=distance_worker,\n",
    "                        args=(wid, queue,\n",
    "                              lookup_table, distance_cache, results, lock,\n",
    "                              items_in_q, items_done, workers_waiting))\n",
    "            for wid in range(NWORKERS)]\n",
    "\n",
    "procs = [reader, *workers]\n",
    "for p in procs: p.start()\n",
    "\n",
    "monitor(TOTAL_FILES, NWORKERS,\n",
    "      files_done, items_in_q, workers_waiting, items_done,\n",
    "      procs)\n",
    "\n",
    "for p in procs: p.join()\n",
    "\n",
    "print(\"\\nPer-worker stats:\", list(results))\n",
    "print(\"Cached distances :\", len(distance_cache))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e71d09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_task6-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
