{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec1d6d9",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "Data\n",
    "\n",
    "The provided dataset contains Automatic Identification System (AIS) data for vessels, including details such as MMSI (Maritime Mobile Service Identity), timestamp, latitude, and longitude. Students will need to calculate the distance traveled by each vessel throughout the day and determine which vessel has the longest route.\n",
    "Tasks\n",
    "\n",
    "    Data Retrieval\n",
    "        Download the dataset from the given URL and unzip it to access the .csv or similar format file contained within.\n",
    "\n",
    "    Data Preparation\n",
    "        Load the data into a PySpark DataFrame.\n",
    "        Ensure that the data types for latitude, longitude, and timestamp are appropriate for calculations and sorting.\n",
    "\n",
    "    Data Processing with PySpark\n",
    "        Calculate the distance between consecutive positions for each vessel using a suitable geospatial library or custom function that can integrate with PySpark.\n",
    "        Aggregate these distances by MMSI to get the total distance traveled by each vessel on that day.\n",
    "\n",
    "    Identifying the Longest Route\n",
    "        Sort or use an aggregation function to determine which vessel traveled the longest distance.\n",
    "\n",
    "    Output\n",
    "        The final output should be the MMSI of the vessel that traveled the longest distance, along with the computed distance.\n",
    "\n",
    "    Code Documentation and Comments\n",
    "        Ensure the code is well-documented, explaining key PySpark transformations and actions used in the process.\n",
    "\n",
    "    Deliverables\n",
    "        A PySpark script that completes the task from loading to calculating and outputting the longest route.\n",
    "        A brief report or set of comments within the code that discusses the findings and any interesting insights about the data or the computation process.\n",
    "\n",
    "Evaluation Criteria\n",
    "\n",
    "    Correct implementation of data loading and preprocessing.\n",
    "    Accuracy of the distance calculation.\n",
    "    Efficiency of PySpark transformations and actions.\n",
    "    Clarity and completeness of documentation and code comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b05c1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/05/29 10:43:34 WARN Utils: Your hostname, DESKTOP-QJASGSB, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/29 10:43:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/29 10:43:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 46886)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/envs/bigdata_task4-3.10/lib/python3.10/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/envs/bigdata_task4-3.10/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/envs/bigdata_task4-3.10/lib/python3.10/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/justas/.pyenv/versions/3.10.15/envs/bigdata_task4-3.10/lib/python3.10/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da073ac2",
   "metadata": {},
   "source": [
    "# Spark examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a87efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f201b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd6266d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533a67f",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd9bd3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/05/29 10:48:02 WARN Utils: Your hostname, DESKTOP-QJASGSB, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/29 10:48:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/29 10:48:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- MMSI: integer (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DataFrame: 999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==>                                                      (1 + 21) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DataFrame after dropping null values: 999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data_path = \"../lab1/data/aisdk-test.csv\"\n",
    "# data_path = \"data/aisdk-2024-05-04.csv\"\n",
    "\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert the '# Timestamp' column to TimestampType\n",
    "df = df.withColumn(\"Timestamp\", to_timestamp(col(\"# Timestamp\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "# Keep only these columns: Timestamp, MMSI, Latitude, Longitude\n",
    "df = df.select(\"Timestamp\", \"MMSI\", \"Latitude\", \"Longitude\")\n",
    "\n",
    "# disply the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# print the length of the DataFrame\n",
    "print(f\"Length of DataFrame: {df.count()}\")\n",
    "\n",
    "# Remove rows with null values in any of the columns\n",
    "df = df.dropna()\n",
    "\n",
    "# print the length of the DataFrame after dropping null values\n",
    "print(f\"Length of DataFrame after dropping null values: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07357441",
   "metadata": {},
   "source": [
    "# Data Processing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadbc102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justas/.pyenv/versions/3.10.15/envs/bigdata_task4-3.10/lib/python3.10/site-packages/pyspark/sql/pandas/functions.py:446: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "EARTH_RADIUS = 6371  # km\n",
    "\n",
    "@pandas_udf(DoubleType(), PandasUDFType.SCALAR)\n",
    "def haversine_udf(lat1, lon1, lat2, lon2):\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return pd.Series(EARTH_RADIUS * c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc24192",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"MMSI\").orderBy(\"Timestamp\")\n",
    "\n",
    "df = df.withColumn(\"prev_lat\", F.lag(\"Latitude\").over(window))\n",
    "df = df.withColumn(\"prev_lon\", F.lag(\"Longitude\").over(window))\n",
    "\n",
    "df = df.withColumn(\"distance\", haversine_udf(\n",
    "    F.col(\"Latitude\"), F.col(\"Longitude\"),\n",
    "    F.col(\"prev_lat\"), F.col(\"prev_lon\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c23881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df.groupBy(\"MMSI\").agg(F.sum(\"distance\").alias(\"total_distance\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff9627ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_route = df_total.orderBy(F.desc(\"total_distance\")).limit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355bdb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|   MMSI|    total_distance|\n",
      "+-------+------------------+\n",
      "|2579999|20065.740571127666|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "longest_route.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796962e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_task4-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
