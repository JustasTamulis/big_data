{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076d0ead",
   "metadata": {},
   "source": [
    "### following mongodb setup\n",
    "\n",
    "https://www.mongodb.com/resources/products/compatibilities/deploying-a-mongodb-cluster-with-docker\n",
    "\n",
    "\n",
    "1) Create docker network with 'docker network create mongoCluster'\n",
    "2) Create mongod instances in docker with\n",
    "3) Create 2 more such instances\n",
    "4) Use pymongo to connect init the replica set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe7ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# connect to the client, everything else is initiated and configured before\n",
    "client = MongoClient('localhost', 27017, directConnection=True, replicaSet='myReplicaSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216517b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongo1:27017 PRIMARY\n",
      "mongo2:27017 SECONDARY\n",
      "mongo3:27017 SECONDARY\n"
     ]
    }
   ],
   "source": [
    "def print_member_status(client):\n",
    "    db = client.admin\n",
    "    rs_status = db.command({'replSetGetStatus': 1})\n",
    "\n",
    "    for m in rs_status['members']:\n",
    "        print(m['name'], m['stateStr'])\n",
    "\n",
    "print_member_status(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ab1a38",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4cd7a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Timestamp</th>\n",
       "      <th>Type of mobile</th>\n",
       "      <th>MMSI</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Navigational status</th>\n",
       "      <th>ROT</th>\n",
       "      <th>SOG</th>\n",
       "      <th>COG</th>\n",
       "      <th>Heading</th>\n",
       "      <th>...</th>\n",
       "      <th>Length</th>\n",
       "      <th>Type of position fixing device</th>\n",
       "      <th>Draught</th>\n",
       "      <th>Destination</th>\n",
       "      <th>ETA</th>\n",
       "      <th>Data source type</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/05/2025 00:00:00</td>\n",
       "      <td>Base Station</td>\n",
       "      <td>2194005</td>\n",
       "      <td>56.344250</td>\n",
       "      <td>4.272000</td>\n",
       "      <td>Unknown value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Surveyed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/05/2025 00:00:00</td>\n",
       "      <td>Base Station</td>\n",
       "      <td>2190064</td>\n",
       "      <td>56.716555</td>\n",
       "      <td>11.519008</td>\n",
       "      <td>Unknown value</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GPS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/05/2025 00:00:00</td>\n",
       "      <td>Class A</td>\n",
       "      <td>244170000</td>\n",
       "      <td>57.848112</td>\n",
       "      <td>10.373403</td>\n",
       "      <td>Under way using engine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>268.7</td>\n",
       "      <td>266.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/05/2025 00:00:00</td>\n",
       "      <td>Class A</td>\n",
       "      <td>265610940</td>\n",
       "      <td>56.893248</td>\n",
       "      <td>12.488332</td>\n",
       "      <td>Under way using engine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.5</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/05/2025 00:00:00</td>\n",
       "      <td>Class A</td>\n",
       "      <td>265610940</td>\n",
       "      <td>56.893248</td>\n",
       "      <td>12.488332</td>\n",
       "      <td>Under way using engine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>302.5</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AIS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           # Timestamp Type of mobile       MMSI   Latitude  Longitude  \\\n",
       "0  01/05/2025 00:00:00   Base Station    2194005  56.344250   4.272000   \n",
       "1  01/05/2025 00:00:00   Base Station    2190064  56.716555  11.519008   \n",
       "2  01/05/2025 00:00:00        Class A  244170000  57.848112  10.373403   \n",
       "3  01/05/2025 00:00:00        Class A  265610940  56.893248  12.488332   \n",
       "4  01/05/2025 00:00:00        Class A  265610940  56.893248  12.488332   \n",
       "\n",
       "      Navigational status  ROT  SOG    COG  Heading  ... Length  \\\n",
       "0           Unknown value  NaN  NaN    NaN      NaN  ...    NaN   \n",
       "1           Unknown value  NaN  NaN    NaN      NaN  ...    NaN   \n",
       "2  Under way using engine  0.0  9.6  268.7    266.0  ...    NaN   \n",
       "3  Under way using engine  0.0  0.0  302.5    120.0  ...    NaN   \n",
       "4  Under way using engine  0.0  0.0  302.5    120.0  ...    NaN   \n",
       "\n",
       "  Type of position fixing device Draught Destination  ETA  Data source type  \\\n",
       "0                       Surveyed     NaN     Unknown  NaN               AIS   \n",
       "1                            GPS     NaN     Unknown  NaN               AIS   \n",
       "2                      Undefined     NaN     Unknown  NaN               AIS   \n",
       "3                      Undefined     NaN     Unknown  NaN               AIS   \n",
       "4                      Undefined     NaN     Unknown  NaN               AIS   \n",
       "\n",
       "    A   B   C   D  \n",
       "0 NaN NaN NaN NaN  \n",
       "1 NaN NaN NaN NaN  \n",
       "2 NaN NaN NaN NaN  \n",
       "3 NaN NaN NaN NaN  \n",
       "4 NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file = \"aisdk_test.csv\"\n",
    "\n",
    "df_sample = pd.read_csv(file, nrows=100) # Reading a small sample just to show structure, tasks will process full file\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47df20b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sample.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25ff90",
   "metadata": {},
   "source": [
    "## Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "362fdf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CSV file: aisdk_test.csv\n",
      "Number of CPU cores available: 22\n",
      "MongoDB Host: localhost:27017, Replica Set: myReplicaSet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "import numpy as np # For NaN checks if needed, and for histogram binning\n",
    "\n",
    "# Configuration\n",
    "DB_HOST = 'localhost'\n",
    "DB_PORT = 27017\n",
    "REPLICA_SET = 'myReplicaSet'  # From existing cell\n",
    "DB_NAME = 'ais_assignment_db'\n",
    "RAW_COLLECTION_NAME = 'ais_raw_data'\n",
    "FILTERED_COLLECTION_NAME = 'ais_filtered_data'\n",
    "CSV_FILE_PATH = file  # Using 'file' variable from cell a2d2fe13 ('aisdk_test.csv')\n",
    "\n",
    "CHUNK_SIZE = 10000 \n",
    "NUM_PROCESSES = mp.cpu_count()\n",
    "\n",
    "print(f\"Using CSV file: {CSV_FILE_PATH}\")\n",
    "print(f\"Number of CPU cores available: {NUM_PROCESSES}\")\n",
    "print(f\"MongoDB Host: {DB_HOST}:{DB_PORT}, Replica Set: {REPLICA_SET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141fe476",
   "metadata": {},
   "source": [
    "## Task 2: Data Insertion in Parallel\n",
    "This task involves reading data from the CSV file (`aisdk_test.csv`) and inserting it into a MongoDB collection (`ais_raw_data`) in parallel. Timestamps are converted to datetime objects. Each worker process uses its own MongoClient instance via the MongoWorker class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e35415f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoWorker:\n",
    "    def __init__(self, db_host, db_port, replica_set, db_name, collection_name):\n",
    "        self.db_host = db_host\n",
    "        self.db_port = db_port\n",
    "        self.replica_set = replica_set\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.client = MongoClient(self.db_host, self.db_port, directConnection=True, replicaSet=self.replica_set)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.collection = self.db[self.collection_name]\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "\n",
    "    def insert_chunk(self, chunk_df):\n",
    "        try:\n",
    "            chunk_df['# Timestamp'] = pd.to_datetime(chunk_df['# Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "            chunk_df.dropna(subset=['# Timestamp'], inplace=True)\n",
    "            records = chunk_df.to_dict(orient='records')\n",
    "            print(len(records), \"records to insert\")\n",
    "            if records:\n",
    "                self.collection.insert_many(records, ordered=False)\n",
    "            return len(records)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in worker during insert_chunk: {e}\")\n",
    "            return 0\n",
    "\n",
    "def process_chunk_for_insertion(chunk_df_tuple):\n",
    "    chunk_df, db_host, db_port, replica_set, db_name, collection_name = chunk_df_tuple\n",
    "    with MongoWorker(db_host, db_port, replica_set, db_name, collection_name) as worker:\n",
    "        return worker.insert_chunk(chunk_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35656ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping collection: ais_raw_data...\n",
      "Collection ais_raw_data dropped.\n"
     ]
    }
   ],
   "source": [
    "with MongoClient(DB_HOST, DB_PORT, directConnection=True, replicaSet=REPLICA_SET) as main_client:\n",
    "    db = main_client[DB_NAME]\n",
    "    print(f\"Dropping collection: {RAW_COLLECTION_NAME}...\")\n",
    "    db[RAW_COLLECTION_NAME].drop()\n",
    "    print(f\"Collection {RAW_COLLECTION_NAME} dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfdd65a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data insertion into ais_raw_data from aisdk_test.csv...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(f\"Starting data insertion into {RAW_COLLECTION_NAME} from {CSV_FILE_PATH}...\")\n",
    "chunk_iterator = pd.read_csv(CSV_FILE_PATH, chunksize=CHUNK_SIZE)\n",
    "tasks = [(chunk, DB_HOST, DB_PORT, REPLICA_SET, DB_NAME, RAW_COLLECTION_NAME) for chunk in chunk_iterator]\n",
    "len(tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3443d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 records to insert\n",
      " records to insert\n",
      "Processed chunk 1, 9999 records inserted. Total so far: 9999\n",
      "Total documents inserted into ais_raw_data: 9999\n",
      "Processed chunk 1, 9999 records inserted. Total so far: 9999\n",
      "Total documents inserted into ais_raw_data: 9999\n"
     ]
    }
   ],
   "source": [
    "total_inserted = 0\n",
    "with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "    for i, inserted_count in enumerate(pool.imap_unordered(process_chunk_for_insertion, tasks)):\n",
    "        total_inserted += inserted_count\n",
    "        print(f\"Processed chunk {i+1}, {inserted_count} records inserted. Total so far: {total_inserted}\")\n",
    "\n",
    "print(f\"Total documents inserted into {RAW_COLLECTION_NAME}: {total_inserted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea2f410e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_inserted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336b144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "Creating indexes on ais_raw_data for MMSI and # Timestamp...\n",
      "Indexes created on raw data collection.\n"
     ]
    }
   ],
   "source": [
    "main_client =  MongoClient(DB_HOST, DB_PORT, directConnection=True, replicaSet=REPLICA_SET)\n",
    "\n",
    "db = main_client[DB_NAME]\n",
    "raw_collection = db[RAW_COLLECTION_NAME]\n",
    "\n",
    "print(raw_collection.count_documents({}))\n",
    "if raw_collection.count_documents({}) > 0:\n",
    "    print(f\"Creating indexes on {RAW_COLLECTION_NAME} for MMSI and # Timestamp...\")\n",
    "    raw_collection.create_index([(\"MMSI\", ASCENDING)])\n",
    "    raw_collection.create_index([(\"# Timestamp\", ASCENDING)])\n",
    "    raw_collection.create_index([(\"MMSI\", ASCENDING), (\"# Timestamp\", ASCENDING)])\n",
    "    print(\"Indexes created on raw data collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b31263",
   "metadata": {},
   "source": [
    "## Task 3: Data Noise Filtering in Parallel\n",
    "This task filters noise from the `ais_raw_data` collection. \n",
    "1. Vessels with less than 100 data points in `ais_raw_data` are identified and excluded.\n",
    "2. For the remaining vessels, data points are filtered based on missing or invalid essential fields: \n",
    "   - Required fields: `MMSI`, `Latitude`, `Longitude`, `ROT`, `SOG`, `COG`, `Heading` (must not be missing/NaN).\n",
    "   - `Navigational status` must not be 'Unknown value' or missing.\n",
    "The filtered data is stored in `ais_filtered_data`. Each worker process handles a distinct MMSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d89459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mmsi_for_filtering(filter_args):\n",
    "    mmsi, db_host, db_port, replica_set, db_name, raw_coll_name, filtered_coll_name = filter_args\n",
    "    with MongoWorker(db_host, db_port, replica_set, db_name, raw_coll_name) as raw_worker:\n",
    "        required_fields = ['MMSI', 'Latitude', 'Longitude', 'ROT', 'SOG', 'COG', 'Heading']\n",
    "        nav_status_field = 'Navigational status'\n",
    "        invalid_nav_status = 'Unknown value'\n",
    "        vessel_data = list(raw_worker.collection.find({'MMSI': mmsi}).sort('# Timestamp', ASCENDING))\n",
    "        \n",
    "        filtered_points_for_mmsi = []\n",
    "        for point in vessel_data:\n",
    "            is_valid_point = True\n",
    "            for field in required_fields:\n",
    "                value = point.get(field)\n",
    "                if value is None or (isinstance(value, float) and np.isnan(value)):\n",
    "                    is_valid_point = False\n",
    "                    break\n",
    "            if not is_valid_point: continue\n",
    "\n",
    "            nav_status = point.get(nav_status_field)\n",
    "            if nav_status is None or nav_status == invalid_nav_status:\n",
    "                is_valid_point = False\n",
    "            \n",
    "            if is_valid_point:\n",
    "                filtered_points_for_mmsi.append(point)\n",
    "        \n",
    "        if filtered_points_for_mmsi:\n",
    "            with MongoWorker(db_host, db_port, replica_set, db_name, filtered_coll_name) as filtered_worker:\n",
    "                filtered_worker.collection.insert_many(filtered_points_for_mmsi, ordered=False)\n",
    "            return len(filtered_points_for_mmsi)\n",
    "        return 0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with MongoClient(DB_HOST, DB_PORT, directConnection=True, replicaSet=REPLICA_SET) as main_client:\n",
    "        db = main_client[DB_NAME]\n",
    "        raw_collection = db[RAW_COLLECTION_NAME]\n",
    "\n",
    "        print(f\"Dropping collection: {FILTERED_COLLECTION_NAME}...\")\n",
    "        db[FILTERED_COLLECTION_NAME].drop()\n",
    "        print(f\"Collection {FILTERED_COLLECTION_NAME} dropped.\")\n",
    "\n",
    "        if raw_collection.count_documents({}) > 0:\n",
    "            print(\"Identifying vessels with >= 100 data points from raw data...\")\n",
    "            pipeline = [\n",
    "                {'$group': {'_id': '$MMSI', 'count': {'$sum': 1}}},\n",
    "                {'$match': {'count': {'$gte': 100}}}\n",
    "            ]\n",
    "            mmsi_docs_to_process = list(raw_collection.aggregate(pipeline))\n",
    "            mmsis_to_filter = [doc['_id'] for doc in mmsi_docs_to_process]\n",
    "            print(f\"Found {len(mmsis_to_filter)} vessels with >= 100 data points to filter.\")\n",
    "\n",
    "            if mmsis_to_filter:\n",
    "                filter_tasks = [(mmsi, DB_HOST, DB_PORT, REPLICA_SET, DB_NAME, RAW_COLLECTION_NAME, FILTERED_COLLECTION_NAME) for mmsi in mmsis_to_filter]\n",
    "                total_filtered_inserted = 0\n",
    "                processed_mmsi_count = 0\n",
    "                with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "                    for i, inserted_count in enumerate(pool.imap_unordered(process_mmsi_for_filtering, filter_tasks)):\n",
    "                        total_filtered_inserted += inserted_count\n",
    "                        processed_mmsi_count +=1\n",
    "                        print(f\"Filtered MMSI {processed_mmsi_count}/{len(mmsis_to_filter)}, {inserted_count} records kept. Total: {total_filtered_inserted}\")\n",
    "                print(f\"Total documents inserted into {FILTERED_COLLECTION_NAME}: {total_filtered_inserted}\")\n",
    "            else:\n",
    "                print(\"No vessels found meeting the >= 100 data points criteria for filtering.\")\n",
    "        else:\n",
    "            print(f\"{RAW_COLLECTION_NAME} is empty. Skipping filtering task.\")\n",
    "\n",
    "    with MongoClient(DB_HOST, DB_PORT, directConnection=True, replicaSet=REPLICA_SET) as main_client:\n",
    "        db = main_client[DB_NAME]\n",
    "        filtered_collection = db[FILTERED_COLLECTION_NAME]\n",
    "        if filtered_collection.count_documents({}) > 0:\n",
    "            print(f\"Creating indexes on {FILTERED_COLLECTION_NAME} for MMSI and # Timestamp...\")\n",
    "            filtered_collection.create_index([(\"MMSI\", ASCENDING)])\n",
    "            filtered_collection.create_index([(\"# Timestamp\", ASCENDING)])\n",
    "            filtered_collection.create_index([(\"MMSI\", ASCENDING), (\"# Timestamp\", ASCENDING)])\n",
    "            print(\"Indexes created on filtered data collection.\")\n",
    "        else:\n",
    "            print(f\"Skipping index creation as {FILTERED_COLLECTION_NAME} is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d043d7",
   "metadata": {},
   "source": [
    "## Task 4: Calculation of Delta t and Histogram Generation\n",
    "This task calculates the time difference (delta t) in milliseconds between subsequent data points for each vessel in the `ais_filtered_data` collection. A histogram of these delta t values is then generated. Each worker process handles delta_t calculation for a distinct MMSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8605e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_t_for_mmsi(delta_t_args):\n",
    "    mmsi, db_host, db_port, replica_set, db_name, filtered_coll_name = delta_t_args\n",
    "    with MongoWorker(db_host, db_port, replica_set, db_name, filtered_coll_name) as worker:\n",
    "        vessel_data = list(worker.collection.find({'MMSI': mmsi}).sort('# Timestamp', ASCENDING))\n",
    "        delta_t_values_for_mmsi = []\n",
    "        if len(vessel_data) > 1:\n",
    "            for i in range(len(vessel_data) - 1):\n",
    "                t1 = vessel_data[i]['# Timestamp']\n",
    "                t2 = vessel_data[i+1]['# Timestamp']\n",
    "                if isinstance(t1, datetime) and isinstance(t2, datetime):\n",
    "                    delta = (t2 - t1).total_seconds() * 1000\n",
    "                    if delta >= 0:\n",
    "                         delta_t_values_for_mmsi.append(delta)\n",
    "                else:\n",
    "                    print(f\"Warning: Non-datetime object for MMSI {mmsi}, idx {i}. t1:{type(t1)}, t2:{type(t2)}\")\n",
    "        return delta_t_values_for_mmsi\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_delta_t_values = []\n",
    "    with MongoClient(DB_HOST, DB_PORT, directConnection=True, replicaSet=REPLICA_SET) as main_client:\n",
    "        db = main_client[DB_NAME]\n",
    "        filtered_collection = db[FILTERED_COLLECTION_NAME]\n",
    "        \n",
    "        if filtered_collection.count_documents({}) == 0:\n",
    "            print(f\"{FILTERED_COLLECTION_NAME} is empty. Skipping delta_t calculation.\")\n",
    "        else:\n",
    "            print(f\"Fetching distinct MMSIs from {FILTERED_COLLECTION_NAME} for delta_t...\")\n",
    "            distinct_mmsis_for_dt = filtered_collection.distinct('MMSI')\n",
    "            print(f\"Found {len(distinct_mmsis_for_dt)} distinct MMSIs for delta_t.\")\n",
    "\n",
    "            if distinct_mmsis_for_dt:\n",
    "                delta_t_tasks = [(mmsi, DB_HOST, DB_PORT, REPLICA_SET, DB_NAME, FILTERED_COLLECTION_NAME) for mmsi in distinct_mmsis_for_dt]\n",
    "                processed_mmsi_dt_count = 0\n",
    "                with mp.Pool(processes=NUM_PROCESSES) as pool:\n",
    "                    for i, dt_list_for_mmsi in enumerate(pool.imap_unordered(calculate_delta_t_for_mmsi, delta_t_tasks)):\n",
    "                        all_delta_t_values.extend(dt_list_for_mmsi)\n",
    "                        processed_mmsi_dt_count +=1\n",
    "                        print(f\"Delta_t for MMSI {processed_mmsi_dt_count}/{len(distinct_mmsis_for_dt)}. Found {len(dt_list_for_mmsi)} values.\")\n",
    "                print(f\"Total delta_t values collected: {len(all_delta_t_values)}\")\n",
    "\n",
    "    if all_delta_t_values:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        positive_delta_t = [dt for dt in all_delta_t_values if dt > 0] # Ensure positive for log scale if used\n",
    "        if not positive_delta_t:\n",
    "            print(\"No positive delta_t values to plot.\")\n",
    "        else:\n",
    "            plt.hist(positive_delta_t, bins=100, edgecolor='black')\n",
    "            plt.yscale('log', nonpositive='clip')\n",
    "            plt.title('Histogram of Delta t values (ms) between AIS data points')\n",
    "            plt.xlabel('Delta t (milliseconds)')\n",
    "            plt.ylabel('Frequency (log scale)')\n",
    "            plt.grid(True, which=\"both\", linestyle='--')\n",
    "            if positive_delta_t:\n",
    "                print(f\"Delta_t stats (ms): Min: {np.min(positive_delta_t):.2f}, Max: {np.max(positive_delta_t):.2f}, Mean: {np.mean(positive_delta_t):.2f}, Median: {np.median(positive_delta_t):.2f}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No delta_t values to plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff63f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_task3-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
